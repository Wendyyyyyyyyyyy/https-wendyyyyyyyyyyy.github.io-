<!doctype html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> EchoProject | Keran Wang </title> <meta name="author" content="Keran Wang"> <meta name="description" content="Video-based AI for beat-to-beat assessment of cardiac function"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?b05f9a0b7405d7c8c89c7465593dea81"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?dadeb9c5d1fd12bc8d37475657446863"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?53a094b51ed1d1e025731eb00d240058" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>" > <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://wendyyyyyyyyyyy.github.io/projects/1_project/"> <script src="/assets/js/theme.js?7b1068a1099d4262cace6933e385240d"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?46af317e693b09921dcb92261d123fbc" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Keran</span> Wang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">EchoProject</h1> <p class="post-description">Video-based AI for beat-to-beat assessment of cardiac function</p> </header> <article> <h2 id="introduction">Introduction</h2> <p>This project is based on the paper “Video-based AI for beat-to-beat assessment of cardiac function,” which presents a novel deep learning algorithm, EchoNet-Dynamic, for the automated evaluation of cardiac function from echocardiogram videos.</p> <h3 id="paper-overview">Paper Overview</h3> <p>Cardiac function is crucial for maintaining normal systemic tissue perfusion. Impairment in cardiac function, often referred to as cardiomyopathy or heart failure, is a leading cause of hospitalization and a significant global health issue. One of the primary metrics for assessing cardiac function is the left ventricular ejection fraction (EF), which is critical for diagnosing cardiovascular diseases, screening for cardiotoxicity, and managing clinical decisions for patients with critical illnesses.</p> <p>However, human assessment of EF involves considerable inter-observer variability and can be labor-intensive, requiring manual tracing of the ventricle size to quantify every beat. To address these challenges, the EchoNet-Dynamic algorithm leverages deep learning techniques to surpass human performance in segmenting the left ventricle, estimating EF, and assessing cardiomyopathy.</p> <h3 id="echonet-dynamic-algorithm">EchoNet-Dynamic Algorithm</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>EchoNet-Dynamic consists of three key components:</p> <ol> <li><strong>Frame-Level Semantic Segmentation</strong>: <ul> <li>Utilizes a convolutional neural network (CNN) with atrous convolutions for frame-level segmentation of the left ventricle.</li> <li>This approach captures larger patterns and generates segmentation throughout the cardiac cycle, providing an interpretable intermediary that mimics the clinical workflow.</li> </ul> </li> <li><strong>Spatiotemporal Convolutions for EF Prediction</strong>: <ul> <li>A CNN model with residual connections and spatiotemporal convolutions predicts EF from the echocardiogram videos.</li> <li>This architecture integrates both spatial and temporal information, enhancing the model’s ability to capture patterns across frames.</li> </ul> </li> <li><strong>Beat-to-Beat Estimation of Cardiac Function</strong>: <ul> <li>The model identifies each cardiac cycle and generates a clip of 32 frames, averaging clip-level EF estimates for each beat.</li> <li>This approach reduces the error in EF estimation by leveraging information across multiple cardiac cycles, allowing for precise diagnosis of cardiovascular diseases in real-time.</li> </ul> </li> </ol> <h3 id="model-performance">Model Performance</h3> <p>EchoNet-Dynamic was trained on a dataset of 10,030 echocardiogram videos obtained from routine clinical practice at Stanford Medicine. The model’s performance was evaluated on an unseen test dataset, achieving a mean absolute error of 4.1%, root mean squared error of 5.3%, and R² of 0.81 compared to human annotations. These results demonstrate that EchoNet-Dynamic’s variance is comparable to or less than that of human experts.</p> <p>In addition to its robust performance on the Stanford test dataset, EchoNet-Dynamic was tested on an external dataset from Cedars-Sinai Medical Center, achieving a mean absolute error of 6.0%, root mean squared error of 7.7%, and R² of 0.77. This cross-hospital reliability highlights the model’s potential for broader clinical application.</p> <h3 id="dataset-availability">Dataset Availability</h3> <p>To promote further innovation, the authors have made publicly available a large dataset of 10,030 annotated echocardiogram videos. This dataset includes expert human tracings, volume estimates, and calculations of the left ventricular EF, providing a valuable resource for the medical machine learning community.</p> <h2 id="project-implementation">Project Implementation</h2> <p>In this project, I reproduced the EF prediction model and the segmentation model from EchoNet-Dynamic. I finetuned the EF prediction model on our dataset. Due to data imbalance, I manually collected more rare data to improve model performance.</p> <h3 id="a4c-ef-results">A4C EF Results</h3> <p><strong>Without segmentation:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>test (one clip) R2:   0.937 (0.889 - 0.962)
test (one clip) MAE:  2.66 (2.18 - 3.19)
test (one clip) RMSE: 3.38 (2.74 - 4.02)
test (all clips) R2:   0.927 (0.882 - 0.952)
test (all clips) MAE:  2.83 (2.28 - 3.39)
test (all clips) RMSE: 3.63 (3.01 - 4.21)
</code></pre></div></div> <p><img src="https://github.com/jerrytamchiho/2023-EchoProject-WendyWang/blob/main/a4c_ef.jpg" alt="A4C EF"/></p> <p><strong>With segmentation:</strong></p> <p>(To reproduce, use the <code class="language-plaintext highlighter-rouge">for_R</code> directory and run the R script.)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; summary(abs(dataNoAugmentation$V3 - dataNoAugmentation$EF))
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.0546  0.9325  2.4867  2.8639  4.2843 10.2367 
&gt; rmse(dataNoAugmentation$V3,dataNoAugmentation$EF) 
[1] 3.6722
&gt; summary(modelNoAugmentation)$r.squared
[1] 0.9285011
</code></pre></div></div> <h3 id="paper-result">Paper Result:</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>summary(abs(dataNoAugmentation$V3 - dataNoAugmentation$EF))
# Mean of 4.216
rmse(dataNoAugmentation$V3,dataNoAugmentation$EF) 
## 5.56
summary(modelNoAugmentation)$r.squared
# 0.79475
</code></pre></div></div> <p>Our model outperforms the paper’s results for the A4C EF task without segmentation.</p> <h3 id="a2c-ef-results">A2C EF Results</h3> <p><strong>Without segmentation:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>test (one clip) R2:   0.934 (0.908 - 0.958)
test (one clip) MAE:  2.75 (2.04 - 3.54)
test (one clip) RMSE: 3.73 (2.54 - 4.91)
test (all clips) R2:   0.931 (0.906 - 0.954)
test (all clips) MAE:  2.85 (2.14 - 3.67)
test (all clips) RMSE: 3.81 (2.71 - 4.89)
</code></pre></div></div> <p><img src="https://github.com/jerrytamchiho/2023-EchoProject-WendyWang/blob/main/a2c_ef.jpg" alt="A2C EF"/></p> <p><strong>With segmentation:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; summary(abs(dataNoAugmentation$V3 - dataNoAugmentation$EF))
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.0392  1.2950  2.4707  3.0739  4.4038 13.9361 
&gt; rmse(dataNoAugmentation$V3,dataNoAugmentation$EF) 
[1] 4.074103
&gt; summary(modelNoAugmentation)$r.squared
[1] 0.9306509
</code></pre></div></div> <h3 id="paper-result-1">Paper Result:</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>summary(abs(dataNoAugmentation$V3 - dataNoAugmentation$EF))
# Mean of 4.216
rmse(dataNoAugmentation$V3,dataNoAugmentation$EF) 
## 5.56
summary(modelNoAugmentation)$r.squared
# 0.79475
</code></pre></div></div> <p>Our model outperforms the paper’s results for the A2C EF task without segmentation.</p> <h3 id="trails">Trails</h3> <p>I conducted 9-fold cross-validation, with R² variations within ±0.04. Hyperparameter tuning showed that unfreezing 3 layers performed best.</p> <h3 id="lav">LAV</h3> <p>I focused on videos with a lower chamber view and used several data augmentation techniques. The best method was training from the last best point.</p> <h3 id="data">Data</h3> <p>Three folders contain almost all data (excluding echonet videos):</p> <p><img src="https://github.com/jerrytamchiho/2023-EchoProject-WendyWang/blob/main/data_folder.jpg" alt="Data Folders"/></p> <p>Old videos are in <code class="language-plaintext highlighter-rouge">EchoNet-A2C/crop_clear_old</code> and <code class="language-plaintext highlighter-rouge">EchoNet-A4C/crop_clear_old</code>, while new EF videos and old videos are in <code class="language-plaintext highlighter-rouge">EchoNet-A2C/crop_clear_all</code> and <code class="language-plaintext highlighter-rouge">EchoNet-A4C/crop_clear_all</code>.</p> <p>Echonet dataset: <a href="https://stanfordaimi.azurewebsites.net/datasets/834e1cd1-92f7-4268-9daa-d359198b310a">Echonet Dataset</a></p> <h2 id="fine-tuning">Fine Tuning</h2> <p>Refer to <a href="https://arxiv.org/abs/1803.09820">this paper</a> for fine-tuning models.</p> <h3 id="lav-a2c-result">LAV A2C Result:</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>test (one clip) R2:   0.784 (0.354 - 0.923)
test (one clip) MAE:  7.88 (5.18 - 11.01)
test (one clip) RMSE: 10.24 (6.68 - 13.61)
test (all clips) R2:   0.785 (0.326 - 0.923)
test (all clips) MAE:  8.10 (5.51 - 10.96)
test (all clips) RMSE: 10.24 (6.86 - 13.51)
</code></pre></div></div> <p><img src="https://github.com/jerrytamchiho/2023-EchoProject-WendyWang/blob/main/a2c_lav.jpg" alt="A2C LAV"/></p> <h3 id="lav-a4c-result">LAV A4C Result:</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>test (one clip) R2:   0.818 (0.632 - 0.901)
test (one clip) MAE:  6.27 (4.46 - 8.19)
test (one clip) RMSE: 8.34 (6.34 - 10.11)
test (all clips) R2:   0.710 (0.331 - 0.915)
test (all clips) MAE:  6.71 (4.29 - 9.74)
test (all clips) RMSE: 10.55 (5.74 - 15.24)
</code></pre></div></div> <p><img src="https://github.com/jerrytamchiho/2023-EchoProject-WendyWang/blob/main/a4c_lav.jpg" alt="A4C LAV"/></p> </article> <h2>References</h2> <div class="publications"> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> &copy; Copyright 2024 Keran Wang. Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?b977fe0c21b2118ed853308b1b923969"></script> <script src="/assets/js/no_defer.js?699fa7cbe3b29f831db7d5250ba3203a"></script> <script defer src="/assets/js/common.js?d3a25b46bbd2e0a751a27b173abc6e5f"></script> <script defer src="/assets/js/copy_code.js?d359581efc54b08366f9ef8219e6e511" type="text/javascript"></script> <script src="/assets/js/bibsearch.js?566de02c0ffca2afee640601367e857c" type="module"></script> <script defer src="/assets/js/jupyter_new_tab.js?25eff8ff4144a010e4ad7b31403102cf"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?66cb7c6fd8c87d040727f1e99683717e"></script> <ninja-keys hideBreadcrumbs noAutoLoadMdIcons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-songs-i-often-listened-to-last-semester",title:"Songs I often listened to last semester",description:"\u5199\u4e8eSiebel (sb)",section:"Posts",handler:()=>{window.location.href="/blog/2024/free-songs-without-ad/"}},{id:"post-\u60f3\u5ff5\u5317\u4eac",title:"\u60f3\u5ff5\u5317\u4eac",description:"\u5199\u4e8eGrainger Library",section:"Posts",handler:()=>{window.location.href="/blog/2024/miss-beijing/"}},{id:"post-from-homer-lake-to-champaign",title:"from Homer Lake to Champaign",description:"sunset and us",section:"Posts",handler:()=>{window.location.href="/blog/2024/happiest-blog/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"news-start-my-exchange-at-the-university-of-illinois-at-urbana-champaign",title:"Start my exchange at the University of Illinois at Urbana-Champaign.",description:"",section:"News"},{id:"news-the-webpage-is-born",title:"The webpage is born.",description:"",section:"News"},{id:"projects-echoproject",title:"EchoProject",description:"Video-based AI for beat-to-beat assessment of cardiac function",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-restaurant-info-application",title:"Restaurant Info Application",description:"a scalable restaurant recommendation web application on Google Cloud Platform",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-big-two-card-game",title:"Big Two card game",description:"implementing networking and multi-threading capabilities in Java, enabling four players to play over the internet",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-friday-board-game",title:"Friday board game",description:"C++-based solo adventure adapted from Friedemann Friese\u2019s famous board game of the same name",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6D%75%6B%65%30%30%39%31%35@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Wendyyyyyyyyyyy","_blank")}},{id:"socials-instagram",title:"Instagram",section:"Socials",handler:()=>{window.open("https://instagram.com/wendy00915","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?f74bfa9a88ab862fb9df1c46146b7b7d"></script> </body> </html>